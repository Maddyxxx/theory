опыт
---------------------------------------------------------
профилирование приложений 		- pprof, trace
автоматизация развертывания приложений  - CI/CD
---------------------------------------------------------
БД
---------------------------------------------------------
Работал с postgre SQL, MS SQL, работал напрямую с запросами в базу через pgx и через GOrm, 
так же создавал индексы
Профилировал и оптимизировал запросы, работал с транзакциями и хранимыми процедурами. 
Немного знаю про шардирование и репликацию, но руками не делал

нереляционные бд:
- кеш редис, еластик документы
- работал как потребитель
- знаю что есть разные стратегии взаимодействия с кешом но не настраивал
+- знаю как внутри работает еластик, как в нём индексируются тексты по лексеммам, но не настраивал
---------------------------------------------------------
тестирование
---------------------------------------------------------
Писал юнит и интеграционные тесты, для юнитов использовался gomock, для интеграционных - использовал подготовленное скриптами окружение, в котором уже запускались (значащие) группы интеграционных тестов, чаще всего
интеграционные запускались в рамках ночного билда группами.
---------------------------------------------------------
написание кода
---------------------------------------------------------
В проектах была стандартная трехслойная архитектура (контроллер, сервис, репозиторий). Где-то была кодогенерация, 
и сервисы создавались по шаблону, где-то мы писали их сами, старались сделать поменьше. На одном из проектов использовали GIN, пользовался Gorilla Mux для роутинга.
---------------------------------------------------------
профилирование
---------------------------------------------------------
Для отладки приложения пользовался стандартными интсрументами golang: go benchmark (race флаг), trace, pprof
Как правило в сервисах были внедрены метрики и хелсчеки.
Иногда приходилось подключаться к приложению в проде дебаггером, на удивление занимало немного ресурсов и не сильно замедляло работу приложения (как я понял, за счет семплирования. sempl)
---------------------------------------------------------
работа с протоколами
---------------------------------------------------------
На всех проектах использовался RPC и где-то был HTTP. На одном проекте HTTP ручками писали сами, и они были доступны сразу из сервиса
На другом проекте мы перенесли HTTP ручки на gateway сервер для доступа извне, а сам он внутри дергал RPC
---------------------------------------------------------
шины данных (kafka, rabbit, sqs, nuts)
---------------------------------------------------------
Работал с kafka и rabbit (не достаточно быстро работает для бигтеха, примерно 10.000 RPS(запросы в секунду) то что заявляет разработчик), но от последнего постепенно отказывались
Я писал как сервисы-консьюмеры, так и продюсеры.
Стандартно ориентируюсь что такое топик(большие группы), партишн(отдельные каналы(жилы) данных), как распределяются сообщения по ключу, что такое консюмер группы.
Самостоятельно поддерживать кафка кластер не могу, но зайти и посмотреть, что происходит и поднять новый партишн, могу.
с кафка работал через внутренние библитиотеки от платформенных команд (под капотом конфлюенд кафка)
---------------------------------------------------------
Rabbit - умная шина данных на которой лежит ответственность за доставку сообщения получателям.
-----------------------------------------------------------
Kafka - журнал в который получатели сами должны приходить и читать нужные им данные

плюс отличие в выдерживаемой нагрузке - Rabbit ~10k сообщений, Kafka за счёт кластеризации сотни тысяч (400-600k)

Это распределенный брокер сообщений.
Асинхронная доставка большого количества сообщений

Миллионы сообщений в секунду
Развертывание - ноды

Торик кафки - журнал событий где хранятся сообщения (работает только на добавление данных). События в журнал неизменны

Продюсер - любой сервис код отправляет данные в кафку
Консюмер - сервис, вычитывающий данные из кафки

---------------------------------------------------------
контейнеризация, docker, cuber, linux
---------------------------------------------------------
SLA
Service Level Agreement = договорённость о сроках выполнения заявок

Grafana - система отображения графиков метрик

Kanban+Scrum
Kanban = методология принятия решений по выбору задач. Ближайший родственник - Scrum

у меня был Kanban разбитый на спринты
---------------------------------------------------------
defer
---------------------------------------------------------
для работы с мьютексами
для логирования конца работы метода
для закрытия канала
для работы с вейтгруппами
работа с внешними коннектами может быть ещё
или graceful shutdown
---------------------------------------------------------
Rest/Nuts
---------------------------------------------------------

набор принципов по постройке системы
rest - структура построения API:
0. клиент-серверная архитектура
1. унификация путей
2. управление системой через ресурсы/объекты
3. отсутствие состояний между запросами
4. возможность кешировать запросы
---------------------------------------------------------
ООП в go
---------------------------------------------------------
- инкапсуляция (области видимости и маленькая/большая буквы)
- полиморфизм (интерфейсы и тайп свитчи)
- декорирование структур или наследование (когда мы одну структуру помещаем внутрь другой и так получаем некое подобие наследования)
- дженерики и рефлексия с кодогенерацией

---------------------------------------------------------
docker
---------------------------------------------------------
docker задачи: 
- создание и модификация образа, 
- передача конфиг файла девопсам, 
- разворачивание образа из регистра, 
- создание локального стенда из нескольких сервисов и шины между ними, 
- поднятие зависимостей для интеграционных тестов
---------------------------------------------------------
kubernetes
---------------------------------------------------------
зайти на деплоймент, снять логи/ошибки, посмотреть конфигурацию, поправить сетевое взаимодействие (например порты)

swagger
---------------------------------------------------------
rest/http/api/rpc
---------------------------------------------------------
строил REST API, могу написать ручки, но в последнее время мы старались переносить все http взаимодействие с фронтом на гейт вэй сервер, а
он уже ко внутренним сервисам ходил по RPC, таким образом мы снижали необходимость поддерживать и генерировать http ручки

- реализация сложной или распределенной асинхронной логики происходит с использованием шин данных и архитектурных шаблонов типа SAGA
	- разделить бизнес логику, транспортный уровень и данные можно с помощью разделения архитектуры на слои и компоненты, трёхслойная
архитектура и гексагональная
	- внутри сервиса архитектура приложения выстроена с разделением на слои презентации, бизнес-слой и слой общения с БД
---------------------------------------------------------
	- если ввести ключи идемпотентности и по ним можно отсеять дважды отправленные запросы, например на создание пользователя в
определенной абстрактной системе
---------------------------------------------------------

- как увольняли предыдущего сотрудника? за что?
- за что поднимают зп?
- 

### **Пример оптимизации производительности в сервисе доставки продуктов**

**Контекст**:
Сервис доставки продуктов обрабатывает **1000+ заказов в секунду**. Основные узкие места:
- **Высокие аллокации** при парсинге JSON (заказы, товары, пользователи).
- **Нагрузка на GC** из-за большого количества временных объектов.
- **Медленные SQL-запросы** при обновлении статусов заказов.

---

## **1. Оптимизация JSON-парсинга (уменьшение аллокаций)**

### **Проблема**
Каждый запрос к API парсит JSON заказа:
```go
type Order struct {
    ID       string    `json:"id"`
    Items    []Item    `json:"items"`
    UserID   string    `json:"user_id"`
    Status   string    `json:"status"`
}

// Парсинг стандартным json.Unmarshal (много аллокаций)
var order Order
err := json.Unmarshal(data, &order)
```

### **Решение: `jsoniter` + пул буферов**
```go
import "github.com/json-iterator/go"

var json = jsoniter.ConfigFastest  // Оптимизированный парсер
var bufPool = sync.Pool{
    New: func() interface{} { return new(bytes.Buffer) },
}

func ParseOrder(data []byte) (Order, error) {
    var order Order
    buf := bufPool.Get().(*bytes.Buffer)
    buf.Reset()
    buf.Write(data)
    err := json.Unmarshal(buf.Bytes(), &order)
    bufPool.Put(buf)
    return order, err
}
```
**Результат**:
- **На 40% меньше аллокаций** (за счёт пула буферов).
- **Ускорение парсинга на 25%** (`jsoniter` быстрее стандартного `encoding/json`).

---

## **2. Оптимизация работы с БД (уменьшение нагрузки на GC)**

### **Проблема**
При массовом обновлении статусов заказов создаётся много временных строк:
```go
for _, order := range orders {
    query := fmt.Sprintf("UPDATE orders SET status='%s' WHERE id='%s'", order.Status, order.ID) // Аллокация строки
    db.Exec(query)
}
```

### **Решение: Prepared Statements + Batch-запросы**
```go
stmt, err := db.Prepare("UPDATE orders SET status=? WHERE id=?")
defer stmt.Close()

batchSize := 100
for i, order := range orders {
    stmt.Exec(order.Status, order.ID)
    if i%batchSize == 0 {
        // Периодическая синхронизация
        if _, err := stmt.Exec(); err != nil {
            log.Printf("Batch error: %v", err)
        }
    }
}
```
**Результат**:
- **На 60% меньше аллокаций** (нет `fmt.Sprintf`).
- **Ускорение БД на 30%** (меньше round-trip-запросов).

---

## **3. Оптимизация кэширования (снижение нагрузки на GC)**

### **Проблема**
Кэш товаров хранится в `map[string]Item` и пересоздаётся каждые 5 минут:
```go
var cache map[string]Item

func UpdateCache() {
    newCache := make(map[string]Item) // Аллокация новой мапы
    for _, item := range fetchItems() {
        newCache[item.ID] = item
    }
    cache = newCache // Старая мапа удаляется GC
}
```

### **Решение: `sync.Map` + переиспользование памяти**
```go
var cache sync.Map // Конкурентно-безопасный кэш

func UpdateCache() {
    items := fetchItems()
    for _, item := range items {
        cache.Store(item.ID, item) // Без аллокаций при обновлении
    }
}
```
**Результат**:
- **На 90% меньше аллокаций** (нет пересоздания мапы).
- **Нулевые паузы GC** (`sync.Map` оптимизирован под частые чтения/записи).

---

## **4. Оптимизация работы с горутинами (уменьшение накладных расходов)**

### **Проблема**
Каждый заказ обрабатывается в новой горутине:
```go
for _, order := range orders {
    go processOrder(order) // 1000+ горутин → нагрузка на планировщик
}
```

### **Решение: Worker Pool (ограничение горутин)**
```go
jobs := make(chan Order, 1000)
results := make(chan error, 1000)

// Запуск 100 воркеров (а не 1000+ горутин)
for i := 0; i < 100; i++ {
    go func() {
        for order := range jobs {
            results <- processOrder(order)
        }
    }()
}

// Отправка задач
for _, order := range orders {
    jobs <- order
}
close(jobs)

// Обработка результатов
for range orders {
    if err := <-results; err != nil {
        log.Printf("Error: %v", err)
    }
}
```
**Результат**:
- **На 80% меньше горутин** → меньше переключений контекста.
- **Стабильная задержка** (нет перегрузки планировщика).

---

## **Итог: что дала оптимизация?**

| Метрика               | До оптимизации | После оптимизации | Улучшение |
|-----------------------|----------------|-------------------|-----------|
| **Аллокации/сек**     | 500 MB         | 150 MB            | **-70%**  |
| **Время GC**          | 500 мс/с       | 100 мс/с          | **-80%**  |
| **RPS (заказы/сек)**  | 1,000          | 2,500             | **+150%** |
| **Задержка 99%**      | 300 мс         | 80 мс             | **-73%**  |

**Вывод**:
- **`jsoniter` + пул буферов** → меньше аллокаций при парсинге.
- **Prepared Statements** → быстрее БД.
- **`sync.Map`** → стабильный кэш без GC.
- **Worker Pool** → контролируемое число горутин.

Эти изменения сделали сервис **быстрее, стабильнее и дешевле в эксплуатации**. 🚀


### **Архитектура микросервисов для доставки продуктов на Go**

#### **1. Разделение на сервисы**
| Сервис              | Описание                          | Технологии          |
|---------------------|-----------------------------------|---------------------|
| **Order Service**   | Создание/отслеживание заказов     | Go, gRPC, PostgreSQL|
| **Payment Service** | Обработка платежей                | Go, REST, Stripe API|
| **Delivery Service**| Логистика и курьеры               | Go, Kafka, Redis    |
| **Notification**    | Уведомления (email/SMS)           | Go, RabbitMQ        |
| **User Service**    | Управление пользователями         | Go, gRPC, MongoDB   |

---

## **2. Взаимодействие между сервисами**

### **🔹 Синхронное (REST/gRPC)**
**Для:** Запросов, требующих немедленного ответа (например, проверка цены).

**Пример (gRPC):**
```go
// order_service.proto
service OrderService {
    rpc CreateOrder (OrderRequest) returns (OrderResponse);
}

// Клиент в Payment Service
conn, _ := grpc.Dial("order-service:50051", grpc.WithInsecure())
client := pb.NewOrderServiceClient(conn)
resp, _ := client.CreateOrder(ctx, &pb.OrderRequest{UserId: "123"})
```

**Плюсы:**
- Типизация (Protocol Buffers).
- Высокая производительность (HTTP/2).

---

### **🔹 Асинхронное (Kafka/RabbitMQ)**
**Для:** Событий, не требующих мгновенной реакции (например, «заказ создан» → уведомление).

**Пример (Kafka):**
```go
// Order Service (отправка события)
producer, _ := sarama.NewSyncProducer([]string{"kafka:9092"}, nil)
producer.SendMessage(&sarama.ProducerMessage{
    Topic: "orders",
    Value: sarama.StringEncoder(`{"event": "order_created", "id": "123"}`),
})

// Delivery Service (потребление)
consumer, _ := sarama.NewConsumer([]string{"kafka:9092"}, nil)
partitionConsumer, _ := consumer.ConsumePartition("orders", 0, sarama.OffsetNewest)
for msg := range partitionConsumer.Messages() {
    var event OrderEvent
    json.Unmarshal(msg.Value, &event)
    // Обработка события...
}
```

**Плюсы:**
- Отказоустойчивость (сообщения не теряются).
- Масштабируемость (партиции Kafka).

---

## **3. Retry-логика**
**Проблема:** Временные ошибки (например, недоступность БД).

### **🔹 Экспоненциальная backoff-стратегия**
```go
func Retry(attempts int, sleep time.Duration, fn func() error) error {
    for i := 0; i < attempts; i++ {
        err := fn()
        if err == nil {
            return nil
        }
        time.Sleep(sleep)
        sleep *= 2 // Увеличиваем задержку
    }
    return fmt.Errorf("после %d попыток: %v", attempts, err)
}

// Использование
err := Retry(3, time.Second, func() error {
    return client.ProcessPayment()
})
```

**Где применять:**
- Вызовы внешних API (Stripe, Google Maps).
- Запросы к БД.

---

## **4. Rate Limiting**
**Проблема:** Защита от перегрузки (например, DDoS или ошибки в коде).

### **🔹 Встроенный `golang.org/x/time/rate`**
```go
limiter := rate.NewLimiter(rate.Every(100*time.Millisecond), 10) // 10 RPS

for {
    if err := limiter.Wait(ctx); err != nil {
        return err
    }
    // Выполняем запрос...
}
```

### **🔹 Продвинутый: Redis + Token Bucket**
```go
// Используем Redis для распределённого лимита
conn := redis.NewClient(&redis.Options{Addr: "redis:6379"})
_, err := conn.Eval(`
    local key = KEYS[1]
    local limit = tonumber(ARGV[1])
    local current = tonumber(redis.call('GET', key) or "0")
    if current + 1 > limit then
        return 0
    else
        redis.call('INCR', key)
        redis.call('EXPIRE', key, 1)
        return 1
    end
`, []string{"user:123"}, 10).Result()
```

**Где применять:**
- Ограничение API для пользователей.
- Защита от спама в уведомлениях.

---

## **5. Мониторинг и трассировка**
**Инструменты:**
- **Prometheus + Grafana** (метрики).
- **Jaeger** (трассировка gRPC/Kafka).
- **Sentry** (ошибки).

**Пример метрики для gRPC:**
```go
grpcRequests := prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: "grpc_requests_total",
        Help: "Количество gRPC-запросов",
    },
    []string{"service", "method"},
)
prometheus.MustRegister(grpcRequests)

// В middleware gRPC-сервера
grpcRequests.WithLabelValues("order_service", "CreateOrder").Inc()
```

---

## **Итог: лучшие практики**
1. **Сервисы**
   - Разделяйте по бизнес-доменам (Order, Payment, Delivery).
   - Используйте **gRPC** для «тяжёлых» внутренних вызовов.

2. **Коммуникация**
   - **REST/gRPC** — для синхронных сценариев.
   - **Kafka** — для событийной модели.

3. **Надёжность**
   - **Retry + backoff** для временных ошибок.
   - **Rate limiting** для защиты инфраструктуры.

4. **Мониторинг**
   - Трассируйте цепочки вызовов между сервисами.
   - Логируйте ключевые события (например, «заказ создан»).

Такая система масштабируется на **100K+ RPS**, отказоустойчива и проста в поддержке